{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🍳  Common recipes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces common recipes you might need while using `sepes` to train/build models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sepes --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Add a leaf to the instance after instantiation.\n",
    "\n",
    "The following recipe, adds a method `add_leaf` that sets a leaf value and name. however, since this method mutate the internal state of the instance `.at['add_leaf']` is used to apply the method functionally and return method call value and a **new** instance ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree(a=1.0, b=2.0, c=3.0, d=4.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sepes as sp\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    a: float = 1.0\n",
    "    b: float = 2.0\n",
    "    c: float = 3.0\n",
    "\n",
    "    def add_leaf(self, name: str, value):\n",
    "        setattr(self, name, value)\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "# Tree(a=1.0, b=2.0, c=3.0)\n",
    "\n",
    "_, tree_with_d = tree.at[\"add_leaf\"](\"d\", 4.0)\n",
    "\n",
    "tree_with_d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Customize optimizers-leaf updates using `sepes` mask + `Optax`.\n",
    "The following recipe, `optax.masked` is used to apply certain optmizers to certain leaves using masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import sepes as sp\n",
    "import jax\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    a: float = 1.0\n",
    "    b: float = 2.0\n",
    "    c: float = 3.0\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "\n",
    "false_mask = tree.at[...].set(False)\n",
    "\n",
    "a_mask = false_mask.at[\"a\"].set(True)\n",
    "b_mask = false_mask.at[\"b\"].set(True)\n",
    "c_mask = false_mask.at[\"c\"].set(True)\n",
    "\n",
    "optim = optax.chain(\n",
    "    # update `a` with sgd of learning rate 1\n",
    "    optax.masked(optax.sgd(learning_rate=1), a_mask),\n",
    "    # update `b` with sgd of learning rate -1\n",
    "    optax.masked(optax.sgd(learning_rate=-1), b_mask),\n",
    "    # update `c` with sgd of learning rate 0\n",
    "    optax.masked(optax.sgd(learning_rate=0), c_mask),\n",
    ")\n",
    "\n",
    "\n",
    "# freeze non-differentiable parameters\n",
    "# in this case all parameters are differentiable\n",
    "# but we do it incase we add a non-differentiable parameter later\n",
    "tree = tree.at[jax.tree_map(sp.is_nondiff, tree)].apply(sp.freeze)\n",
    "\n",
    "optim_state = optim.init(tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Use `numpy` functions on `TreeClass` instance.\n",
    "`jax.numpy` functions can be applied to `TreeClass` instance using a function transformation `bcmap` around the `numpy` function and enabling the feature through `@leafwise`. `@leafwise` additionally enable math operation per-leaf, for example `tree`+1 will add 1 to all leaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree(a=0, b=(0.0, 103.0), c=[104. 105. 106.])\n",
      "Tree(a=1, b=(102.0, 103.0), c=[104. 105. 106.])\n",
      "Tree(a=1, b=(102.0, 103.0), c=[104. 105. 106.])\n"
     ]
    }
   ],
   "source": [
    "import sepes as sp\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "@sp.leafwise\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    a: int = 1\n",
    "    b: tuple[float] = (2.0, 3.0)\n",
    "    c: jax.Array = jnp.array([4.0, 5.0, 6.0])\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "\n",
    "# make where work with arbitrary pytrees\n",
    "tree_where = sp.bcmap(jnp.where)\n",
    "\n",
    "print(tree_where(tree > 2, tree + 100, 0))\n",
    "# Tree(a=0, b=(0.0, 103.0), c=[104. 105. 106.])\n",
    "\n",
    "print(tree.at[tree > 1].apply(lambda x: x + 100))\n",
    "# Tree(a=1, b=(102.0, 103.0), c=[104. 105. 106.])\n",
    "\n",
    "mask = tree_where(tree > 1, True, False)\n",
    "print(tree.at[mask].apply(lambda x: x + 100))\n",
    "# Tree(a=1, b=(102.0, 103.0), c=[104. 105. 106.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] Use visualization tools with arbitrary pytrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list\n",
      "├── [0]=1\n",
      "├── [1]=[...]\n",
      "└── [2]=4\n",
      "list\n",
      "├── [0]=1\n",
      "├── [1]:list\n",
      "│   ├── [0]=2\n",
      "│   └── [1]=3\n",
      "└── [2]=4\n",
      "┌────┬────┬─────┬────┐\n",
      "│Name│Type│Count│Size│\n",
      "├────┼────┼─────┼────┤\n",
      "│[0] │int │1    │    │\n",
      "├────┼────┼─────┼────┤\n",
      "│[1] │list│2    │    │\n",
      "├────┼────┼─────┼────┤\n",
      "│[2] │int │1    │    │\n",
      "├────┼────┼─────┼────┤\n",
      "│Σ   │list│4    │    │\n",
      "└────┴────┴─────┴────┘\n",
      "┌──────┬────┬─────┬────┐\n",
      "│Name  │Type│Count│Size│\n",
      "├──────┼────┼─────┼────┤\n",
      "│[0]   │int │1    │    │\n",
      "├──────┼────┼─────┼────┤\n",
      "│[1][0]│int │1    │    │\n",
      "├──────┼────┼─────┼────┤\n",
      "│[1][1]│int │1    │    │\n",
      "├──────┼────┼─────┼────┤\n",
      "│[2]   │int │1    │    │\n",
      "├──────┼────┼─────┼────┤\n",
      "│Σ     │list│4    │    │\n",
      "└──────┴────┴─────┴────┘\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import sepes as sp\n",
    "\n",
    "tree = [1, [2, 3], 4]\n",
    "\n",
    "print(sp.tree_diagram(tree, depth=1))\n",
    "print(sp.tree_diagram(tree, depth=2))\n",
    "print(sp.tree_summary(tree, depth=1))\n",
    "print(sp.tree_summary(tree, depth=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] Using `on_setattr` to validate/convert inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type and number range check_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On applying Range(min=1, max=100) for field=`in_dim`:\n",
      "0 not in range [1, 100]\n",
      "On applying IsInstance(klass=<class 'int'>) for field=`in_dim`:\n",
      "1.0 not an instance of <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import sepes as sp\n",
    "\n",
    "\n",
    "# you can use any function\n",
    "@sp.autoinit\n",
    "class Range(sp.TreeClass):\n",
    "    min: int | float = -float(\"inf\")\n",
    "    max: int | float = float(\"inf\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not (self.min <= x <= self.max):\n",
    "            raise ValueError(f\"{x} not in range [{self.min}, {self.max}]\")\n",
    "        return x\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class IsInstance(sp.TreeClass):\n",
    "    klass: type | tuple[type, ...]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not isinstance(x, self.klass):\n",
    "            raise TypeError(f\"{x} not an instance of {self.klass}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Foo(sp.TreeClass):\n",
    "    # allow in_dim to be an integer between [1,100]\n",
    "    in_dim: int = sp.field(on_setattr=[IsInstance(int), Range(1, 100)])\n",
    "\n",
    "\n",
    "tree = Foo(1)\n",
    "# no error\n",
    "\n",
    "try:\n",
    "    tree = Foo(0)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    tree = Foo(1.0)\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Array shape and dtype check, then dtype conversion_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On applying ArrayValidator(shape=(3, Ellipsis, 6), dtype=<class 'jax.numpy.float32'>) for field=`array`:\n",
      "Size mismatch, 3 != 1 at dimension 0 \n",
      "\n",
      "On applying ArrayValidator(shape=(3, Ellipsis, 6), dtype=<class 'jax.numpy.float32'>) for field=`array`:\n",
      "Dtype mismatch, array_dtype=dtype('float16') != self.dtype=<class 'jax.numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "import sepes as sp\n",
    "from typing import Any\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class ArrayValidator(sp.TreeClass):\n",
    "    def __init__(self, shape, dtype):\n",
    "        \"\"\"Validate shape and dtype of input array.\n",
    "\n",
    "        Args:\n",
    "            shape: Expected shape of array. available values are int, None, ...\n",
    "                use int for fixed size, None for any size, and ... for any number\n",
    "                of dimensions. for example (..., 1) allows any number of dimensions\n",
    "                with the last dimension being 1. (1, ..., 1) allows any number of\n",
    "                dimensions with the first and last dimensions being 1.\n",
    "            dtype: Expected dtype of array.\n",
    "\n",
    "        Example:\n",
    "            >>> x = jnp.ones((5, 5))\n",
    "            >>> # any number of dimensions with last dim=5\n",
    "            >>> shape = (..., 5)\n",
    "            >>> dtype = jnp.float32\n",
    "            >>> validator = ArrayValidator(shape, dtype)\n",
    "            >>> validator(x)  # no error\n",
    "\n",
    "            >>> # must be 2 dimensions with first dim unconstrained and last dim=5\n",
    "            >>> shape = (None, 5)\n",
    "            >>> validator = ArrayValidator(shape, dtype)\n",
    "            >>> validator(x)  # no error\n",
    "        \"\"\"\n",
    "\n",
    "        if shape.count(...) > 1:\n",
    "            raise ValueError(\"Only one ellipsis allowed\")\n",
    "\n",
    "        for si in shape:\n",
    "            if not isinstance(si, (int, type(...), type(None))):\n",
    "                raise TypeError(f\"Expected int or ..., got {si}\")\n",
    "\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not (hasattr(x, \"shape\") and hasattr(x, \"dtype\")):\n",
    "            raise TypeError(f\"Expected array with shape {self.shape}, got {x}\")\n",
    "\n",
    "        shape = list(self.shape)\n",
    "        array_shape = list(x.shape)\n",
    "        array_dtype = x.dtype\n",
    "\n",
    "        if self.shape and array_dtype != self.dtype:\n",
    "            raise TypeError(f\"Dtype mismatch, {array_dtype=} != {self.dtype=}\")\n",
    "\n",
    "        if ... in shape:\n",
    "            index = shape.index(...)\n",
    "            shape = (\n",
    "                shape[:index]\n",
    "                + [None] * (len(array_shape) - len(shape) + 1)\n",
    "                + shape[index + 1 :]\n",
    "            )\n",
    "\n",
    "        if len(shape) != len(array_shape):\n",
    "            raise ValueError(f\"{len(shape)=} != {len(array_shape)=}\")\n",
    "\n",
    "        for i, (li, ri) in enumerate(zip(shape, array_shape)):\n",
    "            if li is None:\n",
    "                continue\n",
    "            if li != ri:\n",
    "                raise ValueError(f\"Size mismatch, {li} != {ri} at dimension {i}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "# any number of dimensions with firt dim=3 and last dim=6\n",
    "shape = (3, ..., 6)\n",
    "# dtype must be float32\n",
    "dtype = jnp.float32\n",
    "\n",
    "validator = ArrayValidator(shape=shape, dtype=dtype)\n",
    "\n",
    "# convert to half precision from float32\n",
    "converter = lambda x: x.astype(jnp.float16)\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    array: jax.Array = sp.field(on_setattr=[validator, converter])\n",
    "\n",
    "\n",
    "x = jnp.ones([3, 1, 2, 6])\n",
    "tree = Tree(array=x)\n",
    "\n",
    "\n",
    "try:\n",
    "    y = jnp.ones([1, 1, 2, 3])\n",
    "    tree = Tree(array=y)\n",
    "except ValueError as e:\n",
    "    print(e, \"\\n\")\n",
    "    # On applying ArrayValidator(shape=(3, Ellipsis, 6), dtype=<class 'jax.numpy.float32'>) for field=`array`:\n",
    "    # Dtype mismatch, array_dtype=dtype('float16') != self.dtype=<class 'jax.numpy.float32'>\n",
    "\n",
    "try:\n",
    "    z = x.astype(jnp.float16)\n",
    "    tree = Tree(array=z)\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "    # On applying ArrayValidator(shape=(3, Ellipsis, 6), dtype=<class 'jax.numpy.float32'>) for field=`array`:\n",
    "    # Size mismatch, 3 != 1 at dimension 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6] Freeze custom parameters using `.at` manually/with mask\n",
    "\n",
    "In the following example,  some classes like `Dropout`, can contain some leaves that are differentiable,\n",
    "but we do not wish to update them. in `Dropout` Example, the `drop_rate` is a float that\n",
    "should not be updated by optimization. the following recipe shows how to deal with such values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(drop_rate=108.0)\n",
      "Dropout(drop_rate=0.0)\n"
     ]
    }
   ],
   "source": [
    "import sepes as sp\n",
    "import jax\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Dropout(sp.TreeClass):\n",
    "    drop_rate: float = 0.0  # dropout rate, 0 mean no dropout\n",
    "\n",
    "    def __call__(self, x, *, key):\n",
    "        keep_rate = 1.0 - self.drop_rate\n",
    "        mask = jax.random.bernoulli(key, keep_rate, x.shape)\n",
    "        return jnp.where(mask, x / keep_rate, 0.0)\n",
    "\n",
    "\n",
    "x = jnp.arange(10)\n",
    "dropout = Dropout(drop_rate=0.5)\n",
    "dropout(x, key=jax.random.PRNGKey(0))\n",
    "\n",
    "\n",
    "@jax.grad\n",
    "def f(layer: Dropout, x: jax.Array):\n",
    "    return layer(x, key=jax.random.PRNGKey(0)).sum()\n",
    "\n",
    "\n",
    "print(f(dropout, x))\n",
    "# Dropout(drop_rate=108.0)  # <--- this is the gradient which is undesired\n",
    "\n",
    "\n",
    "# lets fix this by freezing the dropout rate\n",
    "@sp.autoinit\n",
    "class Dropout(sp.TreeClass):\n",
    "    drop_rate: float = sp.field(on_setattr=[sp.freeze], default=0.0)\n",
    "\n",
    "    def __call__(self, x, *, key):\n",
    "        keep_rate = 1.0 - self.drop_rate\n",
    "        mask = jax.random.bernoulli(key, keep_rate, x.shape)\n",
    "        return jnp.where(mask, x / keep_rate, 0.0)\n",
    "\n",
    "\n",
    "x = jnp.arange(10)\n",
    "dropout = Dropout(drop_rate=0.5)\n",
    "\n",
    "dropout\n",
    "# Dropout(drop_rate=#0.5)  # -> dropout rate is frozen, to call dropout layer we need to unfreeze it first\n",
    "\n",
    "\n",
    "@jax.grad\n",
    "def f(layer: Dropout, x: jax.Array):\n",
    "    layer = jax.tree_map(sp.unfreeze, layer, is_leaf=sp.is_frozen)\n",
    "    return layer(x, key=jax.random.PRNGKey(0)).sum()\n",
    "\n",
    "\n",
    "f(dropout, x)\n",
    "# Dropout(drop_rate=#0.5)  # <- dropout rate is not updated, can be used safely with optax\n",
    "\n",
    "\n",
    "# lets say, for evaluation we want to set the dropout rate to 0.0\n",
    "# then we can do the following\n",
    "\n",
    "disable_dropout = dropout.at[\"drop_rate\"].set(0.0, is_leaf=sp.is_frozen)\n",
    "print(disable_dropout)\n",
    "# Dropout(drop_rate=0.0)  # now the dropout rate is 0. and unfrozen.\n",
    "# this layer is now safe to use for evaluation without special handling (like eval in pytorch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7] Use `sepes` with `Flax`/`Equinox`\n",
    "The following recipe adds `at` support for `Flax` and `Equinox`. note for equinox use `eqx.Module` instead of `struct.PyTreeNode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlaxTree(a=1, b=(2.0, 3.0), c=f32[3](μ=5.00, σ=0.82, ∈[4.00,6.00]))\n",
      "FlaxTree(a=1, b=(2.0, 3.0), c=[4. 5. 6.])\n",
      "FlaxTree\n",
      "├── .a=1\n",
      "├── .b:tuple\n",
      "│   ├── [0]=2.0\n",
      "│   └── [1]=3.0\n",
      "└── .c=f32[3](μ=5.00, σ=0.82, ∈[4.00,6.00])\n",
      "┌─────┬────────┬─────┬──────┐\n",
      "│Name │Type    │Count│Size  │\n",
      "├─────┼────────┼─────┼──────┤\n",
      "│.a   │int     │1    │      │\n",
      "├─────┼────────┼─────┼──────┤\n",
      "│.b[0]│float   │1    │      │\n",
      "├─────┼────────┼─────┼──────┤\n",
      "│.b[1]│float   │1    │      │\n",
      "├─────┼────────┼─────┼──────┤\n",
      "│.c   │f32[3]  │3    │12.00B│\n",
      "├─────┼────────┼─────┼──────┤\n",
      "│Σ    │FlaxTree│6    │12.00B│\n",
      "└─────┴────────┴─────┴──────┘\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FlaxTree(a=10, b=(2.0, 3.0), c=f32[3](μ=5.00, σ=0.82, ∈[4.00,6.00]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import sepes as sp\n",
    "from flax import struct\n",
    "\n",
    "import jax\n",
    "import sepes as sp\n",
    "from flax import struct\n",
    "\n",
    "# note that flax is registered with `jax.tree_util.register_pytree_with_keys`\n",
    "# otherwise for arbitrary objects you need to do the key registration\n",
    "\n",
    "\n",
    "class FlaxTree(struct.PyTreeNode):\n",
    "    a: int = 1\n",
    "    b: tuple[float] = (2.0, 3.0)\n",
    "    c: jax.Array = jax.numpy.array([4.0, 5.0, 6.0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return sp.tree_repr(self)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return sp.tree_str(self)\n",
    "\n",
    "    @property\n",
    "    def at(self):\n",
    "        return sp.AtIndexer(self)\n",
    "\n",
    "\n",
    "flax_tree = FlaxTree()\n",
    "\n",
    "print(f\"{flax_tree!r}\")\n",
    "print(f\"{flax_tree!s}\")\n",
    "print(sp.tree_diagram(flax_tree))\n",
    "print(sp.tree_summary(flax_tree))\n",
    "\n",
    "flax_tree.at[\"a\"].set(10)\n",
    "# FlaxTree(a=10, b=(2.0, 3.0), c=f32[3](μ=5.00, σ=0.82, ∈[4.00,6.00]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8] `named_parameters()` like in `sepes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NamedSequenceKey(idx=0, name='a'),) 1\n",
      "(NamedSequenceKey(idx=1, name='b'), SequenceKey(idx=0)) 2.0\n",
      "(NamedSequenceKey(idx=1, name='b'), SequenceKey(idx=1)) 3.0\n"
     ]
    }
   ],
   "source": [
    "import sepes as sp\n",
    "import jax\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    a: int = 1\n",
    "    b: tuple[float, float] = (2.0, 3.0)\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "\n",
    "for path, leaf in jax.tree_util.tree_flatten_with_path(tree)[0]:\n",
    "    print(path, leaf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [9] Initialize parameters based on input\n",
    "In this example, a `Linear` layer with a weight parameter based on the shape of the input will be created. Since this requires parameter creation (i.e., `weight`) after instance initialization, we will use `.at` to create a new instance with the added parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer before param is set:\tLazyLinear(out_features=1)\n",
      "Layer after param is set:\tLazyLinear(out_features=1, weight=[[1.]], bias=[0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sepes as sp\n",
    "from typing import Any\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class LazyLinear(sp.TreeClass):\n",
    "    out_features: int\n",
    "\n",
    "    def param(self, name: str, value: Any):\n",
    "        # return the value if it exists, otherwise set it and return it\n",
    "        if name not in vars(self):\n",
    "            setattr(self, name, value)\n",
    "        return vars(self)[name]\n",
    "\n",
    "    def __call__(self, x: jax.Array, *, key: jr.KeyArray = jr.PRNGKey(0)):\n",
    "        weight = self.param(\"weight\", jnp.ones((x.shape[-1], self.out_features)))\n",
    "        bias = self.param(\"bias\", jnp.zeros((self.out_features,)))\n",
    "        return x @ weight + bias\n",
    "\n",
    "\n",
    "x = jnp.ones([10, 1])\n",
    "\n",
    "lazy_linear = LazyLinear(out_features=1)\n",
    "\n",
    "lazy_linear\n",
    "print(f\"Layer before param is set:\\t{lazy_linear}\")\n",
    "\n",
    "\n",
    "# first call will set the parameters\n",
    "_, linear = lazy_linear.at[\"__call__\"](x, key=jr.PRNGKey(0))\n",
    "\n",
    "print(f\"Layer after param is set:\\t{linear}\")\n",
    "# subsequent calls will use the same parameters and not set them again\n",
    "linear(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10] Store intermediate values\n",
    "\n",
    "This example shows how to capture specific intermediate values within each function call in this example."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use state threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate values:\t\n",
      " (Array([[0. ],\n",
      "       [0.5],\n",
      "       [1. ],\n",
      "       [1.5],\n",
      "       [2. ]], dtype=float32), Array([[-0.09999937],\n",
      "       [ 0.40000063],\n",
      "       [ 0.90000063],\n",
      "       [ 1.4000006 ],\n",
      "       [ 1.9000006 ]], dtype=float32))\n",
      "\n",
      "Final tree:\t\n",
      " Tree(a=0.801189)\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import sepes as sp\n",
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    a: float = 1.0\n",
    "\n",
    "    def __call__(self, x: jax.Array, intermediate: tuple[Any, ...]):\n",
    "        x = x + self.a\n",
    "        # store intermediate variables\n",
    "        return x, intermediate + (x,)\n",
    "\n",
    "\n",
    "def loss_func(tree: Tree, x: jax.Array, y: jax.Array, intermediate: tuple[Any, ...]):\n",
    "    ypred, intermediate = tree(x, intermediate)\n",
    "    loss = jnp.mean((ypred - y) ** 2)\n",
    "    return loss, intermediate\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    tree: Tree,\n",
    "    optim_state: optax.OptState,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    intermediate: tuple[Any, ...],\n",
    "):\n",
    "    grads, intermediate = jax.grad(loss_func, has_aux=True)(tree, x, y, intermediate)\n",
    "    updates, optim_state = optim.update(grads, optim_state)\n",
    "    tree = optax.apply_updates(tree, updates)\n",
    "    return tree, optim_state, intermediate\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "optim = optax.adam(1e-1)\n",
    "optim_state = optim.init(tree)\n",
    "\n",
    "x = jnp.linspace(-1, 1, 5)[:, None]\n",
    "y = x**2\n",
    "\n",
    "intermediate = ()\n",
    "\n",
    "for i in range(2):\n",
    "    tree, optim_state, intermediate = train_step(tree, optim_state, x, y, intermediate)\n",
    "\n",
    "\n",
    "print(\"Intermediate values:\\t\\n\", intermediate)\n",
    "print(\"\\nFinal tree:\\t\\n\", tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using [oryx](https://github.com/jax-ml/oryx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate values:\t\n",
      " ({'x': Array([[0. ],\n",
      "       [0.5],\n",
      "       [1. ],\n",
      "       [1.5],\n",
      "       [2. ]], dtype=float32)}, {'x': Array([[-0.09999937],\n",
      "       [ 0.40000063],\n",
      "       [ 0.90000063],\n",
      "       [ 1.4000006 ],\n",
      "       [ 1.9000006 ]], dtype=float32)})\n",
      "\n",
      "Final tree:\t\n",
      " Tree(a=0.801189)\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import sepes as sp\n",
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import oryx\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    a: float = 1.0\n",
    "\n",
    "    def __call__(self, x: jax.Array):\n",
    "        x = x + self.a\n",
    "        # store intermediate variables with oryx\n",
    "        x = oryx.core.sow(x, tag=\"intermediates\", name=\"x\")\n",
    "        return x\n",
    "\n",
    "\n",
    "def loss_func(tree: Tree, x: jax.Array, y: jax.Array):\n",
    "    ypred = tree(x)\n",
    "    loss = jnp.mean((ypred - y) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    tree: Tree,\n",
    "    optim_state: optax.OptState,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "):\n",
    "    grads = jax.grad(loss_func)(tree, x, y)\n",
    "    updates, optim_state = optim.update(grads, optim_state)\n",
    "    tree = optax.apply_updates(tree, updates)\n",
    "    return tree, optim_state\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "optim = optax.adam(1e-1)\n",
    "optim_state = optim.init(tree)\n",
    "\n",
    "x = jnp.linspace(-1, 1, 5)[:, None]\n",
    "y = x**2\n",
    "\n",
    "intermediate = ()\n",
    "\n",
    "train_step_reap = oryx.core.reap(train_step, tag=\"intermediates\")\n",
    "\n",
    "for i in range(2):\n",
    "    intermediate += (train_step_reap(tree, optim_state, x, y),)\n",
    "    tree, optim_state = train_step(tree, optim_state, x, y)\n",
    "\n",
    "\n",
    "print(\"Intermediate values:\\t\\n\", intermediate)\n",
    "print(\"\\nFinal tree:\\t\\n\", tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [11] Create layers from configuration files\n",
    "The next example shows how to use `sepes.bcmap` to loop over a configuration dictionary that defines creation of simple linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(\n",
       "   weight=f32[1,1](μ=0.31, σ=0.00, ∈[0.31,0.31]), \n",
       "   bias=f32[1](μ=0.00, σ=0.00, ∈[0.00,0.00])\n",
       " ),\n",
       " Linear(\n",
       "   weight=f32[2,1](μ=-1.27, σ=0.33, ∈[-1.59,-0.94]), \n",
       "   bias=f32[1](μ=0.00, σ=0.00, ∈[0.00,0.00])\n",
       " ),\n",
       " Linear(\n",
       "   weight=f32[3,1](μ=0.24, σ=0.53, ∈[-0.48,0.77]), \n",
       "   bias=f32[1](μ=0.00, σ=0.00, ∈[0.00,0.00])\n",
       " ),\n",
       " Linear(\n",
       "   weight=f32[4,1](μ=-0.28, σ=0.21, ∈[-0.64,-0.08]), \n",
       "   bias=f32[1](μ=0.00, σ=0.00, ∈[0.00,0.00])\n",
       " )]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sepes as sp\n",
    "import jax\n",
    "\n",
    "\n",
    "class Linear(sp.TreeClass):\n",
    "    def __init__(self, in_dim: int, out_dim: int, *, key: jax.random.KeyArray):\n",
    "        self.weight = jax.random.normal(key, (in_dim, out_dim))\n",
    "        self.bias = jnp.zeros((out_dim,))\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "\n",
    "config = {\n",
    "    # each layer gets a different input dimension\n",
    "    \"in_dim\": [1, 2, 3, 4],\n",
    "    # out_dim is broadcasted to all layers\n",
    "    \"out_dim\": 1,\n",
    "    # each layer gets a different key\n",
    "    \"key\": list(jax.random.split(jax.random.PRNGKey(0), 4)),\n",
    "}\n",
    "\n",
    "\n",
    "# `bcmap` transforms a function that takes a single input into a function that\n",
    "# arbitrary pytree inputs. in case of a single input, the input is broadcasted\n",
    "# to match the tree structure of the first argument\n",
    "# (in our example is a list of 4 inputs)\n",
    "\n",
    "\n",
    "@sp.bcmap\n",
    "def build_layer(in_dim, out_dim, *, key: jax.random.KeyArray):\n",
    "    return Linear(in_dim, out_dim, key=key)\n",
    "\n",
    "\n",
    "build_layer(config[\"in_dim\"], config[\"out_dim\"], key=config[\"key\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [12] Model ensembles using `jax.vmap`\n",
    "In this example, simple `Linear` layers are grouped by their weight on the first axis using `jax.vmap`. This is useful if the different instances of the model are desired to run in a vectorized fashion (model ensemble).\n",
    "\n",
    "For more check [here](http://matpalm.com/blog/ensemble_nets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single input ensemble shape:\t(4, 10, 1)\n",
      "Multi input ensemble shape:\t(4, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import sepes as sp\n",
    "import functools as ft\n",
    "from typing import Generic, TypeVar\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class Batched(Generic[T]):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Linear(sp.TreeClass):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        *,\n",
    "        key: jr.KeyArray,\n",
    "        name: str,\n",
    "    ):\n",
    "        self.weight = jr.normal(key, (in_dim, out_dim))\n",
    "        self.bias = jnp.zeros((out_dim,))\n",
    "        self.name = name  # non-jax type for `tree_mask`/`tree_unmask` demonstration\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "\n",
    "class FNN(sp.TreeClass):\n",
    "    def __init__(self, key: jr.KeyArray):\n",
    "        k1, k2, k3 = jr.split(key, 3)\n",
    "        self.l1 = Linear(1, 10, key=k1, name=\"l1\")\n",
    "        self.l2 = Linear(10, 10, key=k2, name=\"l2\")\n",
    "        self.l3 = Linear(10, 1, key=k3, name=\"l3\")\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        x = self.l1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_ensemble(keys: jr.KeyArray) -> Batched[FNN]:\n",
    "    @jax.vmap\n",
    "    def build_liner(key: jr.KeyArray):\n",
    "        # `jax.vmap` require jax-type return\n",
    "        # so use `tree_mask` on return\n",
    "        return sp.tree_mask(FNN(key=key))\n",
    "\n",
    "    return sp.tree_unmask(build_liner(keys))\n",
    "\n",
    "\n",
    "def run_single_input_ensemble(fnns: Batched[FNN], x: jax.Array):\n",
    "    def run_linear(fnn: FNN):\n",
    "        # `jax.vmap` require jax-type return\n",
    "        # so use `tree_mask` on return\n",
    "        return sp.tree_mask(fnn(x))\n",
    "\n",
    "    return jax.vmap(run_linear)(sp.tree_mask(fnns))\n",
    "\n",
    "\n",
    "def run_multi_input_ensemble(fnns: Batched[FNN], x: Batched[jax.Array]):\n",
    "    def run_linear(fnn: FNN, x: jax.Array):\n",
    "        # `jax.vmap` require jax-type return\n",
    "        # so use `tree_mask` on return\n",
    "        return sp.tree_mask(fnn(x))\n",
    "\n",
    "    return jax.vmap(run_linear)(sp.tree_mask(fnns), x)\n",
    "\n",
    "\n",
    "num_layers = 4\n",
    "keys = jr.split(jr.PRNGKey(0), num_layers)\n",
    "\n",
    "# single input ensemble\n",
    "# e.g. each model in the ensemble gets the same input\n",
    "x = jnp.ones([10, 1])\n",
    "fnns = build_ensemble(keys=keys)\n",
    "y = run_single_input_ensemble(fnns, x)\n",
    "print(f\"Single input ensemble shape:\\t{y.shape}\")\n",
    "\n",
    "# multi input ensemble\n",
    "# e.g. each model in the ensemble gets a different input\n",
    "xs = jnp.stack([x, x * 2, x * 3, x * 4])\n",
    "fnns = build_ensemble(keys=keys)\n",
    "ys = run_multi_input_ensemble(fnns, xs)\n",
    "print(f\"Multi input ensemble shape:\\t{ys.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [13] Functional method chaining.\n",
    "\n",
    "If a certain class has a method that mutate its internal state, then `.at[method_name].__call__(*args,**kwargs)` is used to return a tuple of method return value and a new instance.\n",
    "This example shows how to leverage `.at` to enable method chaining by using the `at` functionality.\n",
    "\n",
    "The objective is to achieve the following pattern in a functional way.\n",
    "\n",
    "```python\n",
    "instance = instance.method1(...).method2(...).method3(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original instance:\t Tree(a=1)\n",
      "new instance:\t\t Tree(a=5)\n"
     ]
    }
   ],
   "source": [
    "import sepes as sp\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    a: int\n",
    "\n",
    "    def _add(self, x):\n",
    "        self.a += x\n",
    "\n",
    "    def add(self, x):\n",
    "        # use `.at` to return the new instance\n",
    "        # and avoid mutating the original instance\n",
    "        _, self = self.at[\"_add\"](x)\n",
    "        # return the new instance and discard the return value\n",
    "        return self\n",
    "\n",
    "    def _mul(self, x):\n",
    "        self.a *= x\n",
    "\n",
    "    def mul(self, x):\n",
    "        # use `.at` to return the new instance\n",
    "        # and avoid mutating the original instance\n",
    "        _, self = self.at[\"_mul\"](x)\n",
    "        # return the new instance and discard the return value\n",
    "        return self\n",
    "\n",
    "\n",
    "tree0 = Tree(a=1)\n",
    "tree1 = tree0.add(1).mul(2).add(1)  # ((1 + 1) * 2) + 1 = 5\n",
    "\n",
    "print(\"original instance:\\t\", tree0)\n",
    "print(\"new instance:\\t\\t\", tree1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [14] Using regular expression masking.\n",
    "In this example, positive values of tree leaves with name starts with `weight_` will be manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree(\n",
      "  weight_1=[  1   4   9  16  25  36  49  64  81 100], \n",
      "  weight_2=[ -1  -2  -3  -4  -5  36  49  64  81 100], \n",
      "  bias=[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import sepes as sp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import re\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    weight_1: jax.Array = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "    weight_2: jax.Array = jnp.array([-1, -2, -3, -4, -5, 6, 7, 8, 9, 10])\n",
    "    bias: jax.Array = jnp.ones(10)\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "\n",
    "positive_mask = jax.tree_map(lambda x: x > 0, tree)  # positive mask\n",
    "tree = tree.at[positive_mask][re.compile(r\"weight_.*\")].apply(lambda x: x**2)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [15] Creating buffers\n",
    "In this example, certain array will be marked as non-trainable using `jax.lax.stop_gradient` and `field`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "Tree(buffer=[0. 0. 0.])\n"
     ]
    }
   ],
   "source": [
    "import sepes as sp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    buffer: jax.Array = sp.field(on_getattr=[jax.lax.stop_gradient])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.buffer**x\n",
    "\n",
    "\n",
    "tree = Tree(buffer=jnp.array([1.0, 2.0, 3.0]))\n",
    "tree(2.0)  # Array([1., 4., 9.], dtype=float32)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(tree, x):\n",
    "    return jnp.sum(tree(x))\n",
    "\n",
    "\n",
    "print(f(tree, 1.0))\n",
    "print(jax.grad(f)(tree, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [16] Creating Frozen fields\n",
    "In this example, field value freezing is done on class level using `on_geatattr`, and `on_setattr`. This effectively hide the field value across `jax` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "Tree(frozen_a=#1)\n"
     ]
    }
   ],
   "source": [
    "import sepes as sp\n",
    "import jax\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    frozen_a: int = sp.field(on_getattr=[sp.unfreeze], on_setattr=[sp.freeze])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.frozen_a + x\n",
    "\n",
    "\n",
    "tree = Tree(frozen_a=1)  # 1 is non-jaxtype\n",
    "# can be used in jax transformations\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(tree, x):\n",
    "    return tree(x)\n",
    "\n",
    "\n",
    "print(f(tree, 1.0))\n",
    "print(jax.grad(f)(tree, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [17] Parameterization\n",
    "\n",
    "In this example, field value is [parameterized](https://pytorch.org/tutorials/intermediate/parametrizations.html) using `on_getattr`,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 1  8  5]\n",
      " [ 2  5 16]]\n"
     ]
    }
   ],
   "source": [
    "import sepes as sp\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def symmetric(array: jax.Array) -> jax.Array:\n",
    "    triangle = jnp.triu(array)  # upper triangle\n",
    "    return triangle + triangle.transpose(-1, -2)\n",
    "\n",
    "\n",
    "@sp.autoinit\n",
    "class Tree(sp.TreeClass):\n",
    "    symmetric_matrix: jax.Array = sp.field(on_getattr=[symmetric])\n",
    "\n",
    "\n",
    "tree = Tree(symmetric_matrix=jnp.arange(9).reshape(3, 3))\n",
    "print(tree.symmetric_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [18] Working on data pipelines\n",
    "\n",
    "In this example, `AtIndexer` is used in similar fashion to [PyFunctional](https://github.com/EntilZha/PyFunctional) to work on general data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sepes import AtIndexer\n",
    "import jax\n",
    "\n",
    "\n",
    "class Transaction:\n",
    "    def __init__(self, reason, amount):\n",
    "        self.reason = reason\n",
    "        self.amount = amount\n",
    "\n",
    "\n",
    "# this example copied from  https://github.com/EntilZha/PyFunctional\n",
    "transactions = [\n",
    "    Transaction(\"github\", 7),\n",
    "    Transaction(\"food\", 10),\n",
    "    Transaction(\"coffee\", 5),\n",
    "    Transaction(\"digitalocean\", 5),\n",
    "    Transaction(\"food\", 5),\n",
    "    Transaction(\"riotgames\", 25),\n",
    "    Transaction(\"food\", 10),\n",
    "    Transaction(\"amazon\", 200),\n",
    "    Transaction(\"paycheck\", -1000),\n",
    "]\n",
    "\n",
    "indexer = AtIndexer(transactions)\n",
    "where = jax.tree_map(lambda x: x.reason == \"food\", transactions)\n",
    "food_cost = indexer[where].reduce(lambda x, y: x + y.amount, initializer=0)\n",
    "food_cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
